# -*- coding: utf-8 -*-
"""Final_projesi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMINdRth88WqWz5i2i8GwDgUvnllqVvT
"""

####### gerekli k√ºt√ºphanelerin y√ºklenmesi ve importlanmasƒ± ###########

!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

import matplotlib.pyplot as plt
import csv
from sklearn.metrics import roc_curve, auc
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score
from joblib import dump, load
from xgboost import XGBClassifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import random
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import vgg19
from transformers import AutoImageProcessor, AutoModelForImageClassification
from google.colab import files
import shutil
from collections import defaultdict
import requests
import time
from tqdm import tqdm

##################### Datasetten ilk 1000 kombinin indirilmesi ########################

# === Dosya Yollarƒ± ===
item_data_path = "/content/drive/MyDrive/item data fln/item_data.txt"
outfit_data_path = "/content/drive/MyDrive/item data fln/outfit_data.txt"

# === 1. item_data.txt'den item_id ‚Üí {cate_id, pic_url} e≈ülemesi olu≈ütur ===
item_dict = {}

with open(item_data_path, 'r') as f:
    for line in f:
        parts = line.strip().split(',')
        if len(parts) < 3:
            continue
        item_id, cate_id, pic_url = parts[:3]
        item_dict[item_id] = {'cate_id': cate_id, 'pic_url': pic_url}

# === 2. outfit_data.txt'den ilk 50 kombini al ===
outfits = []

with open(outfit_data_path, 'r') as f:
    for i, line in enumerate(f):
        if i >= 1000:
            break
        parts = line.strip().split(',')
        if len(parts) < 2:
            continue
        outfit_id = parts[0]
        item_ids = parts[1].split(';')
        outfits.append({'outfit_id': outfit_id, 'item_ids': item_ids})

# === 3. Her kombin i√ßin klas√∂r olu≈ütur ve item resimlerini indir ===
base_dir = 'Downloaded_Outfits'
os.makedirs(base_dir, exist_ok=True)

not_found_items = []

# User-Agent ba≈ülƒ±ƒüƒ±
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

for idx, outfit in enumerate(tqdm(outfits, desc="Outfits")):
    folder_name = f"outfit_{str(idx+1).zfill(3)}"
    outfit_dir = os.path.join(base_dir, folder_name)
    os.makedirs(outfit_dir, exist_ok=True)

    for item_id in outfit['item_ids']:
        if item_id not in item_dict:
            print(f"‚ùå {item_id} item_data.txt i√ßinde yok.")
            not_found_items.append((outfit['outfit_id'], item_id, 'missing_in_item_data'))
            continue

        raw_url = item_dict[item_id]['pic_url']
        img_url = raw_url if raw_url.startswith('http') else 'https:' + raw_url
        img_path = os.path.join(outfit_dir, f"{item_id}.jpg")

        try:
            response = requests.get(img_url, headers=headers, timeout=5)
            if response.status_code == 200:
                with open(img_path, 'wb') as f:
                    f.write(response.content)
            else:
                print(f"‚ùå {item_id} resmi alƒ±namadƒ±. HTTP Kod: {response.status_code}")
                not_found_items.append((outfit['outfit_id'], item_id, f'HTTP {response.status_code}'))
        except Exception as e:
            print(f"‚ùå {item_id} indirilemedi ‚Üí {e}")
            not_found_items.append((outfit['outfit_id'], item_id, str(e)))

        time.sleep(1)  # Her istekte 1 saniye bekle

# === 4. ƒ∞ndirilemeyen item'larƒ± CSV olarak kaydet ===
if not_found_items:
    df_not_found = pd.DataFrame(not_found_items, columns=['outfit_id', 'item_id', 'reason'])
    fail_csv_path = 'failed_downloads.csv'
    df_not_found.to_csv(fail_csv_path, index=False)
    print(f"\nüö® ƒ∞ndirilemeyen item'lar '{fail_csv_path}' dosyasƒ±na kaydedildi.")
else:
    print("\nüéâ T√ºm g√∂rseller ba≈üarƒ±yla indirildi!")

# === 5. ZIP'e sƒ±kƒ±≈ütƒ±r ve indir ===
shutil.make_archive('Downloaded_Outfits', 'zip', 'Downloaded_Outfits')
files.download('Downloaded_Outfits.zip')

#################### Kƒ±yafet kategorisi tahmini i√ßin ml modeli ################################

processor = AutoImageProcessor.from_pretrained("agestau/dummy-fashion-classification")
model = AutoModelForImageClassification.from_pretrained("agestau/dummy-fashion-classification")

#√∂rnek g√∂rrsel y√ºklenmesi
# G√∂rseli y√ºkle
image = Image.open("/content/drive/MyDrive/Downloaded_Outfits/outfit_024/9fffecdfc716343ae4f54a7fdc214377.jpg")  # G√∂rselin yolunu buraya yazƒ±n

# G√∂rseli g√∂ster
plt.imshow(image)
plt.title("G√∂rsel")
plt.show()
# G√∂rseli RGB formatƒ±na d√∂n√º≈üt√ºr
image = image.convert("RGB")

# G√∂rseli i≈ülemciyle (processor) uygun formata d√∂n√º≈üt√ºr
inputs = processor(images=image, return_tensors="pt")

# √ñrnek g√∂rsel i√ßin dummy modelle kategori tahmini yapƒ±lmasƒ±
with torch.no_grad():
    outputs = model(**inputs)

# Tahmin edilen sƒ±nƒ±f
logits = outputs.logits  # Modelin √ßƒ±ktƒ± logitleri
predicted_class_idx = torch.argmax(logits, dim=-1).item()  # En y√ºksek olasƒ±lƒ±ƒüƒ± veren sƒ±nƒ±f

# Sƒ±nƒ±f etiketlerini yazdƒ±r
print(f"Predicted class index: {predicted_class_idx}")

# Modelin sƒ±nƒ±f etiketlerini yazdƒ±rma
class_names = model.config.id2label  # Modelin sƒ±nƒ±f etiketleri

predicted_class_name = class_names[predicted_class_idx]
print(f"Predicted class: {predicted_class_name}")

################## t√ºm kƒ±yafetlerin kategorilerini tahminleme #####################

# Etiketleri yazmak i√ßin dosya a√ß
with open("etiketler.csv", mode="a", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(["image_path", "predicted_class", "score"])  # ba≈ülƒ±klar

    for outfit in sorted(outfit_folders):
        outfit_path = os.path.join(outfit_folder, outfit)
        image_files = [f for f in os.listdir(outfit_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

        for image_file in image_files:
            image_path = os.path.join(outfit_path, image_file)
            image = Image.open(image_path).convert("RGB")

            # G√∂rseli i≈üle
            inputs = processor(images=image, return_tensors="pt")
            with torch.no_grad():
                outputs = model(**inputs)

            logits = outputs.logits
            predicted_class_idx = torch.argmax(logits, dim=-1).item()
            probabilities = torch.softmax(logits, dim=-1)
            predicted_score = probabilities[0][predicted_class_idx].item()
            class_names = model.config.id2label
            predicted_class_name = class_names[predicted_class_idx]

            # Yazdƒ±r + CSV'ye kaydet
            print(f"{outfit} | {image_file} ‚Üí {predicted_class_name} ({predicted_score:.4f})")
            writer.writerow([image_path, predicted_class_name, f"{predicted_score:.4f}"])

############### vgg19 ile kƒ±yafetlerin √∂zellik vekt√∂rlerinin √ßƒ±karƒ±lmasƒ± ####################

# VGG19 modelini y√ºkle (4096 feature i√ßin)
model_vgg = vgg19(pretrained=True)
model_vgg.classifier = torch.nn.Sequential(*list(model_vgg.classifier.children())[:-3])  # 4096 boyutlu feature √ßƒ±kar
model_vgg.eval()

# G√∂rsel √∂n i≈üleme
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# Root klas√∂r
outfit_folder = "/content/drive/MyDrive/Downloaded_Outfits"
outfit_folders = [folder for folder in os.listdir(outfit_folder) if os.path.isdir(os.path.join(outfit_folder, folder))]

records = []

for outfit in sorted(outfit_folders):
    outfit_path = os.path.join(outfit_folder, outfit)
    image_files = [f for f in os.listdir(outfit_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
    for image_file in image_files:
        image_path = os.path.join(outfit_path, image_file)
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"HATA: {image_path} - {e}")
            continue
        input_tensor = preprocess(image).unsqueeze(0)
        with torch.no_grad():
            features = model_vgg(input_tensor)
        features_np = features.squeeze().numpy()  # 1x4096

        row = [outfit, image_file] + features_np.tolist()
        records.append(row)

# Kolon isimleri (2 bilgi s√ºtunu + 4096 feature)
columns = ['outfit_folder', 'image_file'] + [f'feat_{i+1}' for i in range(4096)]

df = pd.DataFrame(records, columns=columns)
df.to_csv("vgg19_features.csv", index=False)
print("Feature extraction ve kayƒ±t tamamlandƒ±!")

########### Uygun kombinlerin √∂zellik vekt√∂rlerinin ortalamasƒ± alƒ±nmasƒ± ve gerekli label'lar ile csv dosyasƒ±na kaydedilmesi ###############

# CSV'leri y√ºkle
df_feat = pd.read_csv("/content/drive/MyDrive/item data fln/vgg19_features.csv")
df_label = pd.read_csv("/content/drive/MyDrive/item data fln/etiketler.csv")

# Dosya adƒ±nƒ± √ßƒ±kar
df_label["image_file"] = df_label["image_path"].apply(lambda x: os.path.basename(str(x)))

# Birle≈ütir
df_merged = pd.merge(df_feat, df_label[["image_file", "predicted_class"]], on="image_file", how="left")

# Kategorileri e≈üle≈ütir
upper_list = ["Topwear", "Loungewear and Nightwear", "Saree"]
lower_list = ["Bottomwear", "Innerwear", "Dress"]
shoe_list = ["Shoes", "Flip Flops", "Sandal"]
bag_list = ["Bags"]
accessory_list = ["Accessories", "Watches"]
hat_list = ["Headwear", "Caps"]

def detect_cat(label):
    if label in upper_list:
        return "upper"
    elif label in lower_list:
        return "lower"
    elif label in shoe_list:
        return "shoe"
    elif label in bag_list:
        return "bag"
    elif label in accessory_list:
        return "accessory"
    elif label in hat_list:
        return "hat"
    else:
        return None

df_merged["category"] = df_merged["predicted_class"].apply(detect_cat)

# Pozitif kombinleri se√ß
selected = []
for outfit, group in df_merged.groupby("outfit_folder"):
    cats = group["category"].tolist()
    combination_type = None
    if all(cat in cats for cat in ["upper", "lower", "shoe"]):
        combination_type = "shoe"
    elif all(cat in cats for cat in ["upper", "lower", "bag"]):
        combination_type = "bag"
    elif all(cat in cats for cat in ["upper", "lower", "accessory"]):
        combination_type = "accessory"
    elif all(cat in cats for cat in ["upper", "lower", "hat"]):
        combination_type = "hat"

    if combination_type:
        upper = group[group["category"] == "upper"].iloc[0]
        lower = group[group["category"] == "lower"].iloc[0]
        third = group[group["category"] == combination_type].iloc[0]
        selected.append({
            "outfit_folder": outfit,
            "upper_feat": upper.filter(like="feat_").values.astype(float),
            "lower_feat": lower.filter(like="feat_").values.astype(float),
            "third_feat": third.filter(like="feat_").values.astype(float),
            "upper_file": upper["image_file"],
            "lower_file": lower["image_file"],
            "third_file": third["image_file"],
            "type": combination_type
        })

# Dataseti olu≈ütur
data_rows = []
outfit_id = 1
neg_id = 1001

for outfit in selected:
    # Pozitif kombin
    full_feat = np.mean([   # Kombin vekt√∂r √∂zelliklerinin ortalmasƒ±nƒ±n alƒ±nmasƒ±
        outfit["upper_feat"],
        outfit["lower_feat"],
        outfit["third_feat"]
    ], axis=0)
    data_rows.append([
        f"outfit_{outfit_id:04d}",  # Kombin numarasƒ±, √ºstgiyim (ismi), altgiyim (ismi), 3.par√ßa (ismi), ortalama vekt√∂r√º (1x4096),1(pozitif kombin)  olacak ≈üekilde kaydediliyor
        outfit["upper_file"],
        outfit["lower_file"],
        outfit["third_file"],
        *full_feat,
        1
    ])
    outfit_id += 1

    # Negatif kombinlerden 2 tane √ºret
    negatives_needed = 2
    negatives_created = 0
    while negatives_created < negatives_needed:
        bad_type = random.choice(["2_upper", "2_lower", "2_shoes", "weird_mixed", "full_random"])
        try:
            if bad_type == "2_upper":  # 2 adet √ºst giyim 1 adet alt giyimden olu≈üan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "upper"])
                p2 = random.choice([s for s in selected if s["type"] == "upper"])
                p3 = random.choice([s for s in selected if s["type"] == "lower"])
            elif bad_type == "2_lower":  # 2 adet alt giyim 1 adet √ºst giyimden olu≈üan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "lower"])
                p2 = random.choice([s for s in selected if s["type"] == "lower"])
                p3 = random.choice([s for s in selected if s["type"] == "hat"])
            elif bad_type == "2_shoes":  # 2 adet ayakkabƒ± 1 adet √ßantadan olu≈üan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "shoe"])
                p2 = random.choice([s for s in selected if s["type"] == "shoe"])
                p3 = random.choice([s for s in selected if s["type"] == "bag"])
            elif bad_type == "weird_mixed":  # 1 adet ≈üapka 1 adet aksesuar ve 1 adet √ßantadan olu≈üan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "hat"])
                p2 = random.choice([s for s in selected if s["type"] == "accessory"])
                p3 = random.choice([s for s in selected if s["type"] == "bag"])
            else:
                parts = random.sample(selected, 3)  # 3 par√ßanƒ±n da rastgele ≈üekilde se√ßildiƒüi negatif kombin
                p1, p2, p3 = parts

            neg_feat = np.mean([
                p1["upper_feat"],
                p2["lower_feat"],
                p3["third_feat"]
            ], axis=0)
            data_rows.append([
                f"outfit_{neg_id:04d}", # Kombin numarasƒ±, √ºstgiyim (ismi), altgiyim (ismi), 3.par√ßa (ismi), ortalama vekt√∂r√º (1x4096),0(negatif kombin)  olacak ≈üekilde kaydediliyor
                p1["upper_file"],          # burda par√ßalarƒ± deƒüi≈ütirdik fakat yinede √ºstgiyim, altgiyim ve 3.par√ßa oalrak kaydediyoruz.
                p2["lower_file"],
                p3["third_file"],
                *neg_feat,
                0
            ])
            neg_id += 1
            negatives_created += 1
        except:
            continue  # par√ßa bulunamazsa tekrar dene

# OLu≈üturduƒüumuz yeni dataseti csv'ye kaydediyoruz
columns = ["outfit_name", "upper_file", "lower_file", "third_file"] + [f"feat_{i+1}" for i in range(4096)] + ["label"]
df_out = pd.DataFrame(data_rows, columns=columns)
df_out.to_csv("outfit_compatibility_3piece.csv", index=False)

# Olu≈üturulan toplam kombin sayƒ±larƒ±nƒ± yazdƒ±rma
print(f"Toplam pozitif kombin: {outfit_id - 1}")
print(f"Toplam negatif kombin: {neg_id - 1001}")
print(f"Toplam √∂rnek: {len(df_out)}")

# Label'larƒ± yazdƒ±r
labels = df_out["label"]
print(labels)

############ Modeli indirme ##########

!pip install xgboost

########## Model i√ßin veriyi hazƒ±rlama #################

# Veriyi y√ºkle
df = pd.read_csv("outfit_compatibility_3piece.csv")

# √ñzellik ve etiketleri ayƒ±r
X = df.drop(["label", "outfit_name", "upper_file", "lower_file", "third_file"], axis=1).values
y = df["label"].values

# Veriyi √∂l√ßekle
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Eƒüitim/test ayƒ±r
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

########## XGBoost classifier i√ßin kullanƒ±lan hiperparametre ayarlarƒ± ##############

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.01,
    subsample=0.9,
    colsample_bytree=0.8,
    scale_pos_weight=(sum(y == 0) / sum(y == 1)),  # class weight
    eval_metric='logloss',
    random_state=42
)

xgb.fit(X_train, y_train)

######## Model performans deƒüerlendirmesi ve kaydedilmesi  ##################

# Tahmin olasƒ±lƒ±klarƒ±
y_probs = xgb.predict_proba(X_test)[:, 1]

# ROC AUC
roc = roc_auc_score(y_test, y_probs)
print(f"ROC-AUC: {roc:.4f}")

# En iyi threshold'u F1 skora g√∂re se√ß
thresholds = np.linspace(0, 1, 100)
f1s = [f1_score(y_test, y_probs > t) for t in thresholds]
best_threshold = thresholds[np.argmax(f1s)]
print(f"Optimal threshold: {best_threshold:.2f}, F1-score: {max(f1s):.3f}")

# Tahmin ve Confusion Matrix
y_pred = (y_probs > best_threshold).astype(int)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# F1 Skor grafiƒüi
plt.plot(thresholds, f1s)
plt.xlabel("Threshold")
plt.ylabel("F1 Score")
plt.title("Threshold vs F1 Score")
plt.grid(True)
plt.show()


# Modeli kaydet
dump(xgb, "model_xgboost.joblib")

############## Model performansƒ±nƒ± ROC eƒürisi ile deƒüerlendirme ###############

# Modeli y√ºkle
xgb_model = load("/content/model_xgboost.joblib")

# Ger√ßek etiketler (y_test) ve tahmin edilen olasƒ±lƒ±klar (y_probs)
# Burada y_test ve y_probs zaten var diye varsayƒ±yoruz
# y_probs, modelin pozitif sƒ±nƒ±f (1) olasƒ±lƒ±klarƒ±nƒ± i√ßerir
y_probs = xgb_model.predict_proba(X_test)[:, 1]  # 1. s√ºtun pozitif sƒ±nƒ±fƒ±n (1) olasƒ±lƒ±ƒüƒ±

# ROC eƒürisini hesapla
fpr, tpr, thresholds = roc_curve(y_test, y_probs)  # False Positive Rate, True Positive Rate, Thresholds
roc_auc = auc(fpr, tpr)  # ROC eƒürisinin altƒ±ndaki alan (AUC)

# ROC eƒürisini √ßiz
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Rastgele tahminler i√ßin referans √ßizgisi
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()