# -*- coding: utf-8 -*-
"""Final_projesi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uMINdRth88WqWz5i2i8GwDgUvnllqVvT
"""

####### gerekli kÃ¼tÃ¼phanelerin yÃ¼klenmesi ve importlanmasÄ± ###########

!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

import matplotlib.pyplot as plt
import csv
from sklearn.metrics import roc_curve, auc
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score
from joblib import dump, load
from xgboost import XGBClassifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import random
from PIL import Image
import torch
import torchvision.transforms as transforms
from torchvision.models import vgg19
from transformers import AutoImageProcessor, AutoModelForImageClassification
from google.colab import files
import shutil
from collections import defaultdict
import requests
import time
from tqdm import tqdm

##################### Datasetten ilk 1000 kombinin indirilmesi ########################

# === Dosya YollarÄ± ===
item_data_path = "/content/drive/MyDrive/item data fln/item_data.txt"
outfit_data_path = "/content/drive/MyDrive/item data fln/outfit_data.txt"

# === 1. item_data.txt'den item_id â†’ {cate_id, pic_url} eÅŸlemesi oluÅŸtur ===
item_dict = {}

with open(item_data_path, 'r') as f:
    for line in f:
        parts = line.strip().split(',')
        if len(parts) < 3:
            continue
        item_id, cate_id, pic_url = parts[:3]
        item_dict[item_id] = {'cate_id': cate_id, 'pic_url': pic_url}

# === 2. outfit_data.txt'den ilk 50 kombini al ===
outfits = []

with open(outfit_data_path, 'r') as f:
    for i, line in enumerate(f):
        if i >= 1000:
            break
        parts = line.strip().split(',')
        if len(parts) < 2:
            continue
        outfit_id = parts[0]
        item_ids = parts[1].split(';')
        outfits.append({'outfit_id': outfit_id, 'item_ids': item_ids})

# === 3. Her kombin iÃ§in klasÃ¶r oluÅŸtur ve item resimlerini indir ===
base_dir = 'Downloaded_Outfits'
os.makedirs(base_dir, exist_ok=True)

not_found_items = []

# User-Agent baÅŸlÄ±ÄŸÄ±
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

for idx, outfit in enumerate(tqdm(outfits, desc="Outfits")):
    folder_name = f"outfit_{str(idx+1).zfill(3)}"
    outfit_dir = os.path.join(base_dir, folder_name)
    os.makedirs(outfit_dir, exist_ok=True)

    for item_id in outfit['item_ids']:
        if item_id not in item_dict:
            print(f"âŒ {item_id} item_data.txt iÃ§inde yok.")
            not_found_items.append((outfit['outfit_id'], item_id, 'missing_in_item_data'))
            continue

        raw_url = item_dict[item_id]['pic_url']
        img_url = raw_url if raw_url.startswith('http') else 'https:' + raw_url
        img_path = os.path.join(outfit_dir, f"{item_id}.jpg")

        try:
            response = requests.get(img_url, headers=headers, timeout=5)
            if response.status_code == 200:
                with open(img_path, 'wb') as f:
                    f.write(response.content)
            else:
                print(f"âŒ {item_id} resmi alÄ±namadÄ±. HTTP Kod: {response.status_code}")
                not_found_items.append((outfit['outfit_id'], item_id, f'HTTP {response.status_code}'))
        except Exception as e:
            print(f"âŒ {item_id} indirilemedi â†’ {e}")
            not_found_items.append((outfit['outfit_id'], item_id, str(e)))

        time.sleep(1)  # Her istekte 1 saniye bekle

# === 4. Ä°ndirilemeyen item'larÄ± CSV olarak kaydet ===
if not_found_items:
    df_not_found = pd.DataFrame(not_found_items, columns=['outfit_id', 'item_id', 'reason'])
    fail_csv_path = 'failed_downloads.csv'
    df_not_found.to_csv(fail_csv_path, index=False)
    print(f"\nðŸš¨ Ä°ndirilemeyen item'lar '{fail_csv_path}' dosyasÄ±na kaydedildi.")
else:
    print("\nðŸŽ‰ TÃ¼m gÃ¶rseller baÅŸarÄ±yla indirildi!")

# === 5. ZIP'e sÄ±kÄ±ÅŸtÄ±r ve indir ===
shutil.make_archive('Downloaded_Outfits', 'zip', 'Downloaded_Outfits')
files.download('Downloaded_Outfits.zip')

#################### KÄ±yafet kategorisi tahmini iÃ§in ml modeli ################################

processor = AutoImageProcessor.from_pretrained("agestau/dummy-fashion-classification")
model = AutoModelForImageClassification.from_pretrained("agestau/dummy-fashion-classification")

#Ã¶rnek gÃ¶rrsel yÃ¼klenmesi
# GÃ¶rseli yÃ¼kle
image = Image.open("/content/drive/MyDrive/Downloaded_Outfits/outfit_024/9fffecdfc716343ae4f54a7fdc214377.jpg")  # GÃ¶rselin yolunu buraya yazÄ±n

# GÃ¶rseli gÃ¶ster
plt.imshow(image)
plt.title("GÃ¶rsel")
plt.show()
# GÃ¶rseli RGB formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
image = image.convert("RGB")

# GÃ¶rseli iÅŸlemciyle (processor) uygun formata dÃ¶nÃ¼ÅŸtÃ¼r
inputs = processor(images=image, return_tensors="pt")

# Ã–rnek gÃ¶rsel iÃ§in dummy modelle kategori tahmini yapÄ±lmasÄ±
with torch.no_grad():
    outputs = model(**inputs)

# Tahmin edilen sÄ±nÄ±f
logits = outputs.logits  # Modelin Ã§Ä±ktÄ± logitleri
predicted_class_idx = torch.argmax(logits, dim=-1).item()  # En yÃ¼ksek olasÄ±lÄ±ÄŸÄ± veren sÄ±nÄ±f

# SÄ±nÄ±f etiketlerini yazdÄ±r
print(f"Predicted class index: {predicted_class_idx}")

# Modelin sÄ±nÄ±f etiketlerini yazdÄ±rma
class_names = model.config.id2label  # Modelin sÄ±nÄ±f etiketleri

predicted_class_name = class_names[predicted_class_idx]
print(f"Predicted class: {predicted_class_name}")

################## tÃ¼m kÄ±yafetlerin kategorilerini tahminleme #####################

# Etiketleri yazmak iÃ§in dosya aÃ§
with open("etiketler.csv", mode="a", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(["image_path", "predicted_class", "score"])  # baÅŸlÄ±klar

    for outfit in sorted(outfit_folders):
        outfit_path = os.path.join(outfit_folder, outfit)
        image_files = [f for f in os.listdir(outfit_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

        for image_file in image_files:
            image_path = os.path.join(outfit_path, image_file)
            image = Image.open(image_path).convert("RGB")

            # GÃ¶rseli iÅŸle
            inputs = processor(images=image, return_tensors="pt")
            with torch.no_grad():
                outputs = model(**inputs)

            logits = outputs.logits
            predicted_class_idx = torch.argmax(logits, dim=-1).item()
            probabilities = torch.softmax(logits, dim=-1)
            predicted_score = probabilities[0][predicted_class_idx].item()
            class_names = model.config.id2label
            predicted_class_name = class_names[predicted_class_idx]

            # YazdÄ±r + CSV'ye kaydet
            print(f"{outfit} | {image_file} â†’ {predicted_class_name} ({predicted_score:.4f})")
            writer.writerow([image_path, predicted_class_name, f"{predicted_score:.4f}"])

############### vgg19 ile kÄ±yafetlerin Ã¶zellik vektÃ¶rlerinin Ã§Ä±karÄ±lmasÄ± ####################

# VGG19 modelini yÃ¼kle (4096 feature iÃ§in)
model_vgg = vgg19(pretrained=True)
model_vgg.classifier = torch.nn.Sequential(*list(model_vgg.classifier.children())[:-3])  # 4096 boyutlu feature Ã§Ä±kar
model_vgg.eval()

# GÃ¶rsel Ã¶n iÅŸleme
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

# Root klasÃ¶r
outfit_folder = "/content/drive/MyDrive/Downloaded_Outfits"
outfit_folders = [folder for folder in os.listdir(outfit_folder) if os.path.isdir(os.path.join(outfit_folder, folder))]

records = []

for outfit in sorted(outfit_folders):
    outfit_path = os.path.join(outfit_folder, outfit)
    image_files = [f for f in os.listdir(outfit_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
    for image_file in image_files:
        image_path = os.path.join(outfit_path, image_file)
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"HATA: {image_path} - {e}")
            continue
        input_tensor = preprocess(image).unsqueeze(0)
        with torch.no_grad():
            features = model_vgg(input_tensor)
        features_np = features.squeeze().numpy()  # 1x4096

        row = [outfit, image_file] + features_np.tolist()
        records.append(row)

# Kolon isimleri (2 bilgi sÃ¼tunu + 4096 feature)
columns = ['outfit_folder', 'image_file'] + [f'feat_{i+1}' for i in range(4096)]

df = pd.DataFrame(records, columns=columns)
df.to_csv("vgg19_features.csv", index=False)
print("Feature extraction ve kayÄ±t tamamlandÄ±!")

########### Uygun kombinlerin Ã¶zellik vektÃ¶rlerinin ortalamasÄ± alÄ±nmasÄ± ve gerekli label'lar ile csv dosyasÄ±na kaydedilmesi ###############

# CSV'leri yÃ¼kle
df_feat = pd.read_csv("/content/drive/MyDrive/item data fln/vgg19_features.csv")
df_label = pd.read_csv("/content/drive/MyDrive/item data fln/etiketler.csv")

# Dosya adÄ±nÄ± Ã§Ä±kar
df_label["image_file"] = df_label["image_path"].apply(lambda x: os.path.basename(str(x)))

# BirleÅŸtir
df_merged = pd.merge(df_feat, df_label[["image_file", "predicted_class"]], on="image_file", how="left")

# Kategorileri eÅŸleÅŸtir
upper_list = ["Topwear", "Loungewear and Nightwear", "Saree"]
lower_list = ["Bottomwear", "Innerwear", "Dress"]
shoe_list = ["Shoes", "Flip Flops", "Sandal"]
bag_list = ["Bags"]
accessory_list = ["Accessories", "Watches"]
hat_list = ["Headwear", "Caps"]

def detect_cat(label):
    if label in upper_list:
        return "upper"
    elif label in lower_list:
        return "lower"
    elif label in shoe_list:
        return "shoe"
    elif label in bag_list:
        return "bag"
    elif label in accessory_list:
        return "accessory"
    elif label in hat_list:
        return "hat"
    else:
        return None

df_merged["category"] = df_merged["predicted_class"].apply(detect_cat)

# Pozitif kombinleri seÃ§
selected = []
for outfit, group in df_merged.groupby("outfit_folder"):
    cats = group["category"].tolist()
    combination_type = None
    if all(cat in cats for cat in ["upper", "lower", "shoe"]):
        combination_type = "shoe"
    elif all(cat in cats for cat in ["upper", "lower", "bag"]):
        combination_type = "bag"
    elif all(cat in cats for cat in ["upper", "lower", "accessory"]):
        combination_type = "accessory"
    elif all(cat in cats for cat in ["upper", "lower", "hat"]):
        combination_type = "hat"

    if combination_type:
        upper = group[group["category"] == "upper"].iloc[0]
        lower = group[group["category"] == "lower"].iloc[0]
        third = group[group["category"] == combination_type].iloc[0]
        selected.append({
            "outfit_folder": outfit,
            "upper_feat": upper.filter(like="feat_").values.astype(float),
            "lower_feat": lower.filter(like="feat_").values.astype(float),
            "third_feat": third.filter(like="feat_").values.astype(float),
            "upper_file": upper["image_file"],
            "lower_file": lower["image_file"],
            "third_file": third["image_file"],
            "type": combination_type
        })

# Dataseti oluÅŸtur
data_rows = []
outfit_id = 1
neg_id = 1001

for outfit in selected:
    # Pozitif kombin
    full_feat = np.mean([   # Kombin vektÃ¶r Ã¶zelliklerinin ortalmasÄ±nÄ±n alÄ±nmasÄ±
        outfit["upper_feat"],
        outfit["lower_feat"],
        outfit["third_feat"]
    ], axis=0)
    data_rows.append([
        f"outfit_{outfit_id:04d}",  # Kombin numarasÄ±, Ã¼stgiyim (ismi), altgiyim (ismi), 3.parÃ§a (ismi), ortalama vektÃ¶rÃ¼ (1x4096),1(pozitif kombin)  olacak ÅŸekilde kaydediliyor
        outfit["upper_file"],
        outfit["lower_file"],
        outfit["third_file"],
        *full_feat,
        1
    ])
    outfit_id += 1

    # Negatif kombinlerden 2 tane Ã¼ret
    negatives_needed = 2
    negatives_created = 0
    while negatives_created < negatives_needed:
        bad_type = random.choice(["2_upper", "2_lower", "2_shoes", "weird_mixed", "full_random"])
        try:
            if bad_type == "2_upper":  # 2 adet Ã¼st giyim 1 adet alt giyimden oluÅŸan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "upper"])
                p2 = random.choice([s for s in selected if s["type"] == "upper"])
                p3 = random.choice([s for s in selected if s["type"] == "lower"])
            elif bad_type == "2_lower":  # 2 adet alt giyim 1 adet Ã¼st giyimden oluÅŸan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "lower"])
                p2 = random.choice([s for s in selected if s["type"] == "lower"])
                p3 = random.choice([s for s in selected if s["type"] == "hat"])
            elif bad_type == "2_shoes":  # 2 adet ayakkabÄ± 1 adet Ã§antadan oluÅŸan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "shoe"])
                p2 = random.choice([s for s in selected if s["type"] == "shoe"])
                p3 = random.choice([s for s in selected if s["type"] == "bag"])
            elif bad_type == "weird_mixed":  # 1 adet ÅŸapka 1 adet aksesuar ve 1 adet Ã§antadan oluÅŸan negatif kombin
                p1 = random.choice([s for s in selected if s["type"] == "hat"])
                p2 = random.choice([s for s in selected if s["type"] == "accessory"])
                p3 = random.choice([s for s in selected if s["type"] == "bag"])
            else:
                parts = random.sample(selected, 3)  # 3 parÃ§anÄ±n da rastgele ÅŸekilde seÃ§ildiÄŸi negatif kombin
                p1, p2, p3 = parts

            neg_feat = np.mean([
                p1["upper_feat"],
                p2["lower_feat"],
                p3["third_feat"]
            ], axis=0)
            data_rows.append([
                f"outfit_{neg_id:04d}", # Kombin numarasÄ±, Ã¼stgiyim (ismi), altgiyim (ismi), 3.parÃ§a (ismi), ortalama vektÃ¶rÃ¼ (1x4096),0(negatif kombin)  olacak ÅŸekilde kaydediliyor
                p1["upper_file"],          # burda parÃ§alarÄ± deÄŸiÅŸtirdik fakat yinede Ã¼stgiyim, altgiyim ve 3.parÃ§a oalrak kaydediyoruz.
                p2["lower_file"],
                p3["third_file"],
                *neg_feat,
                0
            ])
            neg_id += 1
            negatives_created += 1
        except:
            continue  # parÃ§a bulunamazsa tekrar dene

# OLuÅŸturduÄŸumuz yeni dataseti csv'ye kaydediyoruz
columns = ["outfit_name", "upper_file", "lower_file", "third_file"] + [f"feat_{i+1}" for i in range(4096)] + ["label"]
df_out = pd.DataFrame(data_rows, columns=columns)
df_out.to_csv("outfit_compatibility_3piece.csv", index=False)

# OluÅŸturulan toplam kombin sayÄ±larÄ±nÄ± yazdÄ±rma
print(f"Toplam pozitif kombin: {outfit_id - 1}")
print(f"Toplam negatif kombin: {neg_id - 1001}")
print(f"Toplam Ã¶rnek: {len(df_out)}")

# Label'larÄ± yazdÄ±r
labels = df_out["label"]
print(labels)

############ Modeli indirme ##########

!pip install xgboost

########## Model iÃ§in veriyi hazÄ±rlama #################

# Veriyi yÃ¼kle
df = pd.read_csv("outfit_compatibility_3piece.csv")

# Ã–zellik ve etiketleri ayÄ±r
X = df.drop(["label", "outfit_name", "upper_file", "lower_file", "third_file"], axis=1).values
y = df["label"].values

# Veriyi Ã¶lÃ§ekle
scaler = StandardScaler()
X = scaler.fit_transform(X)

# EÄŸitim/test ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

########## XGBoost classifier iÃ§in kullanÄ±lan hiperparametre ayarlarÄ± ##############

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=10,
    learning_rate=0.01,
    subsample=0.9,
    colsample_bytree=0.8,
    scale_pos_weight=(sum(y == 0) / sum(y == 1)),  # class weight
    eval_metric='logloss',
    random_state=42
)

xgb.fit(X_train, y_train)

######## Model performans deÄŸerlendirmesi ve kaydedilmesi  ##################

# Tahmin olasÄ±lÄ±klarÄ±
y_probs = xgb.predict_proba(X_test)[:, 1]

# ROC AUC
roc = roc_auc_score(y_test, y_probs)
print(f"ROC-AUC: {roc:.4f}")

# En iyi threshold'u F1 skora gÃ¶re seÃ§
thresholds = np.linspace(0, 1, 100)
f1s = [f1_score(y_test, y_probs > t) for t in thresholds]
best_threshold = thresholds[np.argmax(f1s)]
print(f"Optimal threshold: {best_threshold:.2f}, F1-score: {max(f1s):.3f}")

# Tahmin ve Confusion Matrix
y_pred = (y_probs > best_threshold).astype(int)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# F1 Skor grafiÄŸi
plt.plot(thresholds, f1s)
plt.xlabel("Threshold")
plt.ylabel("F1 Score")
plt.title("Threshold vs F1 Score")
plt.grid(True)
plt.show()


# Modeli kaydet
dump(xgb, "model_xgboost.joblib")

############## Model performansÄ±nÄ± ROC eÄŸrisi ile deÄŸerlendirme ###############

# Modeli yÃ¼kle
xgb_model = load("/content/model_xgboost.joblib")

# GerÃ§ek etiketler (y_test) ve tahmin edilen olasÄ±lÄ±klar (y_probs)
# Burada y_test ve y_probs zaten var diye varsayÄ±yoruz
# y_probs, modelin pozitif sÄ±nÄ±f (1) olasÄ±lÄ±klarÄ±nÄ± iÃ§erir
y_probs = xgb_model.predict_proba(X_test)[:, 1]  # 1. sÃ¼tun pozitif sÄ±nÄ±fÄ±n (1) olasÄ±lÄ±ÄŸÄ±

# ROC eÄŸrisini hesapla
fpr, tpr, thresholds = roc_curve(y_test, y_probs)  # False Positive Rate, True Positive Rate, Thresholds
roc_auc = auc(fpr, tpr)  # ROC eÄŸrisinin altÄ±ndaki alan (AUC)

# ROC eÄŸrisini Ã§iz
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Rastgele tahminler iÃ§in referans Ã§izgisi
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()